{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244826ce",
   "metadata": {
    "id": "244826ce"
   },
   "outputs": [],
   "source": [
    "# DeepSeek-R1 Sentiment Scoring Pipeline using Ollama\n",
    "# Full script version (can be used in .py file or adapted into a notebook)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695953d",
   "metadata": {
    "id": "a695953d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_GPU = True  # Toggle between CPU and GPU\n",
    "MODEL_NAME = \"deepseek_sentiment\"\n",
    "INPUT_PATH = r\"/Users/seemablatif/Library/CloudStorage/GoogleDrive-seemab.latif@seecs.edu.pk/.shortcut-targets-by-id/1qHspO5MC2YvuRdtMkSxFIni55C_DBf69/FinRL/Data/Filtered_News/3K_2018_Summary_Replaced_5000_part1.csv\"\n",
    "SCORED_DIR = r\"/Users/seemablatif/Library/CloudStorage/GoogleDrive-seemab.latif@seecs.edu.pk/.shortcut-targets-by-id/1qHspO5MC2YvuRdtMkSxFIni55C_DBf69/FinRL/Data/Input/Test_One\"  # Output directory for file\n",
    "FAILED_LOG = os.path.join(SCORED_DIR, \"failed_responses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d327f",
   "metadata": {
    "id": "488d327f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === SET OLLAMA GPU/CPU MODE ===\n",
    "def set_ollama_gpu_mode(use_gpu=True):\n",
    "    config_path = os.path.expanduser(\"~/.ollama/config\")\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    config_content = \"[performance]\\noffload = {}\\n\".format(str(use_gpu).lower())\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    print(f\"Ollama GPU mode set to: {'GPU' if use_gpu else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce6088",
   "metadata": {
    "id": "06ce6088"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === LLM FUNCTION ===\n",
    "def get_sentiment(symbol, *articles, retries=2):\n",
    "    articles = [text for text in articles if text != 0 and pd.notnull(text)]\n",
    "    if not articles:\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    failed = []\n",
    "\n",
    "    few_shots = \"\"\"\n",
    "Calculate the sentiment score for the given input:\n",
    "Consider the stock symbol ‘APPL’ and the news article  ‘Below is Validea's guru fundamental report for APPLE INC (AAPL)...’\n",
    "Output\n",
    "{\"score\": 4, \"reason\": \"The news highlights that Apple Inc. (AAPL)...\"}\n",
    "\n",
    "Calculate the sentiment score for the given input:\n",
    "Consider the stock symbol ‘EBAY’ and the news article  ‘Fool.com contributor Parkev Tatevosian reveals his top dividend stocks...’\n",
    "Output\n",
    "{\"score\": 3, \"reason\": \"The news article discusses dividend stock recommendations...\"}\n",
    "\n",
    "Calculate the sentiment score for the given input:\n",
    "Consider the stock symbol ‘AAPL’ and the news article  ‘In a letter to the Department of Justice, Senator Ron Wyden said...’\n",
    "Output\n",
    "{\"score\": 1, \"reason\": \"The news highlights potential privacy concerns...\"}\n",
    "\"\"\"\n",
    "\n",
    "    for article in articles:\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "Forget all previous instructions.\n",
    "\n",
    "You are a financial expert with extensive experience in stock recommendation and market based news sentiment analysis. You will receive summarized news for a specific stock and its stock symbol.\n",
    "Your task is to analyze the overall news in the context of the stock’s potential short-term movement, and assign a sentiment score from 1 to 5, based on the expected directional impact on the company’s stock price. The scoring criteria lies in one of the following score bands:\n",
    "Scoring Criteria:\n",
    "Score 5 – Strongly Positive: The news is likely to significantly increase investor confidence and drive the stock price up.\n",
    "Score 4 – Somewhat Positive: The news is moderately positive, potentially causing a small upward price movement.\n",
    "Score 3 – Neutral: The news is balanced or has no clear market impact.\n",
    "Score 2 – Somewhat Negative: The news may cause a small decline in stock price.\n",
    "Score 1 – Strongly Negative: The news is likely to significantly decrease investor confidence and drive the stock price down.\n",
    "\n",
    "Below are a few examples for reference:\n",
    "{few_shots}\n",
    "\n",
    "IMPORTANT:\n",
    "Only return a Python dictionary in the following format (no commentary or additional explanation):\n",
    "\n",
    "python\n",
    "{{\"score\": <int>, \"reason\": \"<your explanation>\"}}\n",
    "Here is the article:\n",
    "\\\"\\\"\\\"{article}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "                response = ollama.chat(model=\"deepseek-r1\", messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt.strip()}\n",
    "                ])\n",
    "                content = response['message']['content'].strip()\n",
    "\n",
    "                match = re.search(r'\\{.*?\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = match.group(0)\n",
    "                    result_dict = ast.literal_eval(parsed)\n",
    "                    score = result_dict.get(\"score\")\n",
    "                    reason = result_dict.get(\"reason\")\n",
    "                    if isinstance(score, int) and 1 <= score <= 5 and isinstance(reason, str):\n",
    "                        print(f\"Response: {parsed}\")\n",
    "                        results.append((score, reason))\n",
    "                        break\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid score or reason format\")\n",
    "                else:\n",
    "                    raise ValueError(\"No dictionary found in response\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == retries:\n",
    "                    failed.append(article)\n",
    "                    results.append((\"null\", \"null\"))\n",
    "\n",
    "    if failed:\n",
    "        log_df = pd.DataFrame({\"Symbol\": symbol, \"Failed_Article\": failed})\n",
    "        if os.path.exists(FAILED_LOG):\n",
    "            log_df.to_csv(FAILED_LOG, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            log_df.to_csv(FAILED_LOG, index=False)\n",
    "\n",
    "    scores, reasons = zip(*results) if results else ([], [])\n",
    "    return scores, reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bd8e2",
   "metadata": {
    "id": "625bd8e2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === RUN ON FULL CSV ===\n",
    "def run_sentiment_on_full_df(df, output_dir, model_used=MODEL_NAME, batch_size=10):\n",
    "    df = df[pd.notnull(df['Article'])].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No valid articles in the dataset.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Scoring full dataset ({len(df)} rows)\")\n",
    "\n",
    "    sentiment_col = model_used\n",
    "    reason_col = model_used.replace(\"sentiment\", \"reasoning\")\n",
    "\n",
    "    out_file = os.path.join(output_dir, f\"_dataset_scored_part1.csv\")\n",
    "    if os.path.exists(out_file):\n",
    "        scored_df = pd.read_csv(out_file)\n",
    "        processed_indices = scored_df.index[scored_df[sentiment_col].notnull()].tolist()\n",
    "        print(f\"Resuming from previously processed {len(processed_indices)} rows...\")\n",
    "    else:\n",
    "        scored_df = df.copy()\n",
    "        scored_df[sentiment_col] = \"null\"\n",
    "        scored_df[reason_col] = \"null\"\n",
    "        processed_indices = []\n",
    "\n",
    "    for i in tqdm(range(0, len(scored_df), batch_size), desc=\"Scoring dataset\", unit=\"batch\"):\n",
    "        batch_indices = list(range(i, min(i + batch_size, len(scored_df))))\n",
    "        if all(idx in processed_indices for idx in batch_indices):\n",
    "            continue\n",
    "\n",
    "        batch = scored_df.iloc[batch_indices]\n",
    "        articles = batch['Article'].tolist()\n",
    "        scores, reasons = get_sentiment(\"ALL\", *articles)\n",
    "\n",
    "        for idx, score, reason in zip(batch_indices, scores, reasons):\n",
    "            scored_df.at[idx, sentiment_col] = score\n",
    "            scored_df.at[idx, reason_col] = reason\n",
    "\n",
    "        try:\n",
    "            scored_df.to_csv(out_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save CSV in batch: {e}\")\n",
    "\n",
    "        print(f\"Saved progress after batch ending at row {batch_indices[-1]}\")\n",
    "\n",
    "    print(f\"\\nCompleted scoring full dataset. Final CSV saved at: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377336af",
   "metadata": {
    "id": "377336af",
    "outputId": "d691d3df-79a4-4f26-cc77-f778cf375697"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === MAIN EXECUTION ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_ollama_gpu_mode(USE_GPU)\n",
    "    os.makedirs(SCORED_DIR, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "    df.columns = df.columns.str.capitalize()\n",
    "\n",
    "    run_sentiment_on_full_df(df, SCORED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb4c4e",
   "metadata": {
    "id": "73cb4c4e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
